{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valy3124/Data-Science-Medical-DL/blob/main/Lab_04_Transformer_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "ZMggAzUJlRbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmAmEWVnZg9k"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import json\n",
        "from functools import partial\n",
        "from PIL import Image\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers for image classification\n",
        "\n",
        "Transformers have been originally proposed to process sets since it is a permutation-equivariant architecture, i.e., producing the same output permuted if the input is permuted. To apply Transformers to sequences, we have simply added a positional encoding to the input feature vectors, and the model learned by itself what to do with it. So, why not do the same thing on images? This is exactly what [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) proposed in their paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\". Specifically, the Vision Transformer is a model for image classification that views images as sequences of smaller patches. As a preprocessing step, we split an image of, for example, $48\\times 48$ pixels into 9 $16\\times 16$ patches. Each of those patches is considered to be a \"word\"/\"token\" and projected to a feature space. With adding positional encodings and a token for classification on top, we can apply a Transformer as usual to this sequence and start training it for our task. A nice GIF visualization of the architecture is shown below (figure credit - [Phil Wang](https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif\" width=\"600px\"></center>\n",
        "\n",
        "We will walk step by step through the Vision Transformer, and implement all parts by ourselves. First, let's implement the image preprocessing: an image of size $N\\times N$ has to be split into $(N/M)^2$ patches of size $M\\times M$. These represent the input words to the Transformer."
      ],
      "metadata": {
        "id": "R1SOKRf6oDK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"../data\"\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
        "                                     ])\n",
        "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
        "                                     ])\n",
        "# Loading the training dataset. We need to split it into a training and validation part\n",
        "# We need to do a little trick because the validation set should not use the augmentation.\n",
        "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
        "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
        "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
        "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
        "\n",
        "# Loading the test set\n",
        "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
        "\n",
        "# We define a set of data loaders that we can use for various purposes later.\n",
        "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
        "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
        "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
        "\n",
        "# Visualize some examples\n",
        "NUM_IMAGES = 4\n",
        "CIFAR_images = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=4, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "QlBTc8Nif5s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "        flatten_channels - If True, the patches will be returned in a flattened format\n",
        "                           as a feature vector instead of a image grid.\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
        "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
        "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
        "    if flatten_channels:\n",
        "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
        "    return x"
      ],
      "metadata": {
        "id": "wTjjqUSYfwrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)\n",
        "\n",
        "fig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14,3))\n",
        "fig.suptitle(\"Images as input sequences of patches\")\n",
        "for i in range(CIFAR_images.shape[0]):\n",
        "    print(img_patches[i].shape)\n",
        "    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=64, normalize=True, pad_value=0.9)\n",
        "    img_grid = img_grid.permute(1, 2, 0)\n",
        "    ax[i].imshow(img_grid)\n",
        "    ax[i].axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "oQJJJyxsfy1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a transformer from scratch\n",
        "\n",
        "Your task is to implement the key components of the transformer from scratch. Later, you can just use an existing implementation from huggingface or pytorch.\n",
        "\n",
        "Check the course for more insights."
      ],
      "metadata": {
        "id": "9TB58eWkoq07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dmodel, dropout = 0.0, head_dim = 32, qkv_bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dmodel = dmodel\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "        # num_heads is dynamic, we compute it based on dmodel and head_dim\n",
        "        # we want the head dim to be the same regardless of dmodel (see LLaMa paper and other modern architectures)\n",
        "        self.num_heads = dmodel // head_dim\n",
        "\n",
        "        # TODO\n",
        "        # self.to_q = nn.Linear(...)\n",
        "        # self.to_k = nn.Linear(...)\n",
        "        # self.to_v = nn.Linear(...)\n",
        "\n",
        "        self.out_proj = nn.Linear(dmodel, dmodel)  # Optional Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = head_dim**-0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, _ = x.shape\n",
        "\n",
        "        # Shape: (b, num_tokens, d_out)\n",
        "\n",
        "        # TODO\n",
        "        # keys = ...\n",
        "        # queries = ...\n",
        "        # values = ...\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # TODO Normalize attention scores by sqrt(dhead) and then compute softmax on the last dim\n",
        "        # attn_weights = ...\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where dmodel = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.dmodel)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n"
      ],
      "metadata": {
        "id": "icJht-9khcuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, dmodel, dropout=0.0):\n",
        "        super().__init__()\n",
        "        # hidden state is dmodel * 4, it's just a best practice\n",
        "\n",
        "        #self.fc1 = nn.Linear(...)\n",
        "        #self.fc2 = nn.Linear(...)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.gelu(x) # gelu, not relu, it works strictly better\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "D0G0X2gdhgoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dmodel, dropout=0.0, head_dim=32, qkv_bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = MultiHeadAttention(dmodel, dropout, head_dim, qkv_bias)\n",
        "        self.mlp = MLP(dmodel, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dmodel)\n",
        "        self.norm2 = nn.LayerNorm(dmodel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # residual connections + layernorms\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "TkCTFVgOhi50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, dmodel, n_layers, max_seq_len, n_classes, dropout=0.0, head_dim=32, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.dmodel = dmodel\n",
        "        self.n_layers = n_layers\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = head_dim\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # preamble, project input data into the transformer dimmension dmodel\n",
        "        self.input_projection = nn.Linear(input_dim, dmodel)\n",
        "\n",
        "        # encode positions\n",
        "        self.absolute_positional_embeddings = nn.Embedding(max_seq_len, dmodel)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(dmodel, dropout, head_dim, qkv_bias)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # add a special token that we're going to use for classification\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dmodel))\n",
        "        self.classification_output = nn.Linear(dmodel, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = img_to_patch(x, patch_size = 4, flatten_channels = True)\n",
        "        b, num_tokens, _ = x.shape\n",
        "\n",
        "        # project input data into the transformer dimmension dmodel\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # add positions\n",
        "        positions = torch.arange(num_tokens, device = x.device)\n",
        "        positions = self.absolute_positional_embeddings(positions)\n",
        "        x = x + positions\n",
        "\n",
        "        # concatenate the CLS token\n",
        "        cls_token = self.cls_token.expand(b, -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        # send to transformer\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # get the final CLS token from the output\n",
        "        x = x[:, 0]\n",
        "\n",
        "        # do the classification\n",
        "        x = self.classification_output(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "aewA5vufhbuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, device, optimizer, criterion, epoch):\n",
        "    # We set the model to be in training mode\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    # This is only for showing the progress bar\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "\n",
        "    # We iterate through all batches - 1 step is 1 batch of batch_size images\n",
        "    for step, (images, labels) in bar:\n",
        "        # We take the images and their labels and push them on GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Obtain predictions\n",
        "        pred = model(images)\n",
        "\n",
        "        # Compute loss for this batch\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        # Compute gradients for each weight (backpropagation)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights based on gradients (gradient descent)\n",
        "        optimizer.step()\n",
        "\n",
        "        # We keep track of the average training loss\n",
        "        total_train_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_train_loss / dataset_size, 2)\n",
        "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss)\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "def valid_epoch(model, dataloader, device, criterion, epoch):\n",
        "    # We set the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    total_val_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    # We keep track of correct predictions\n",
        "    correct = 0\n",
        "\n",
        "    # This is only for showing the progress bar\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "\n",
        "    for step, (images, labels) in bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        # The raw output of the model is a score for each class\n",
        "        # We keep the index of the class with the highest score as the prediction\n",
        "        _, predicted = torch.max(pred, 1)\n",
        "\n",
        "        # We see how many predictions match the ground truth labels\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # We compute evaluation metrics - loss and accurarcy\n",
        "        total_val_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_val_loss / dataset_size, 2)\n",
        "\n",
        "        accuracy = np.round(100 * correct / dataset_size, 2)\n",
        "\n",
        "        bar.set_postfix(Epoch=epoch, Valid_Acc=accuracy, Valid_Loss=epoch_loss)\n",
        "\n",
        "    return accuracy, epoch_loss\n",
        "\n",
        "\n",
        "def run_training(model, num_epochs, learning_rate, trainloader, testloader):\n",
        "    # Define criterion\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Check if we are using GPU\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    # For keeping track of the best validation accuracy\n",
        "    top_accuracy = 0.0\n",
        "\n",
        "    # We train the emodel for a number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss = train_epoch(model, trainloader, device, optimizer, criterion, epoch)\n",
        "\n",
        "        # For validation we do not keep track of gradients\n",
        "        with torch.no_grad():\n",
        "            val_accuracy, val_loss = valid_epoch(model, testloader, device, criterion, epoch)\n",
        "            if val_accuracy > top_accuracy:\n",
        "                print(f\"Validation Accuracy Improved ({top_accuracy} ---> {val_accuracy})\")\n",
        "                top_accuracy = val_accuracy\n",
        "        print()"
      ],
      "metadata": {
        "id": "NT2rQjijgSQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = VisionTransformerEncoder(\n",
        "    input_dim = 48, # with patch_size of 4 and 3 channels -> 4*4*3 = 48\n",
        "    max_seq_len = 64, # 64 patches\n",
        "    n_classes = 10, # CIFAR-10 has 10 classes\n",
        "    dmodel = 128, # keep it a power of 2. Try bigger values and see how the accuracy changes. But modify the learning rate accordingly!\n",
        "    n_layers = 4, # same thing as above\n",
        "    dropout = 0.01,\n",
        "    head_dim = 16, # whatever, just don't make it too small (... or too big)\n",
        ")\n",
        "\n",
        "summary(vit, input_size = (1, 3, 32, 32))"
      ],
      "metadata": {
        "id": "SLoq5bKalcEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You may want to change the hyperparameters to obtain better results\n",
        "run_training(model=vit, num_epochs = 10, learning_rate=0.001, trainloader = train_loader, testloader = val_loader)"
      ],
      "metadata": {
        "id": "BwXZwPe8hsIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other TODOs\n",
        "\n",
        "# 1. Instead of our hand-made attention mechanism, try to incorporate the optimized version from PyTorch\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "\n",
        "# 1. Instead of our hand-made transformer, try to incorporate the TransformerEncoder class from PyTorch\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html\n",
        "# Make sure to do the same things as we did to make it work for image classification: position embeddings, cls token, patchification etc."
      ],
      "metadata": {
        "id": "HKsVY_lbjrt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5tL2UORulQmW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}